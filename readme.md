# ğŸ¤– RL Bipedal Walking Robot

A complete reinforcement learning (RL) framework for training a bipedal robot to walk using **ROS 2**, **Gazebo**, and **PyTorch (PPO)**.

This project combines **robot simulation** with **deep reinforcement learning**, enabling training, evaluation, and visualization of walking behaviors for a custom bipedal robot.

---

## ğŸ“‚ Project Structure

```
rl-bipedal-walking/
â”œâ”€â”€ ros2_ws/                          # ROS2 workspace
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ bipedal_robot_description/
â”‚           â”œâ”€â”€ launch/               # Launch files
â”‚           â”œâ”€â”€ urdf/                 # Robot description
â”‚           â”œâ”€â”€ worlds/               # Gazebo worlds
â”‚           â”œâ”€â”€ config/               # RViz configs
â”‚           â”œâ”€â”€ scripts/              # ROS2 robot control scripts
â”‚           â”œâ”€â”€ package.xml
â”‚           â””â”€â”€ CMakeLists.txt
â”œâ”€â”€ src/                              # RL Python package
â”‚   â””â”€â”€ rl_bipedal_walking/
â”‚       â”œâ”€â”€ environments/             # Custom Gym environments
â”‚       â”œâ”€â”€ agents/                   # RL agents (PPO)
â”‚       â”œâ”€â”€ training/                 # Training scripts
â”‚       â”œâ”€â”€ evaluation/               # Evaluation tools
â”‚       â”œâ”€â”€ visualization/            # Plotting utilities
â”‚       â”œâ”€â”€ models/                   # Saved trained models
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ scripts/                          # Helper bash scripts
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd rl-bipedal-walking

# Install dependencies
make install

# Setup ROS2 workspace
make setup
```

### 2ï¸âƒ£ Build ROS2 Packages

```bash
make build
```

### 3ï¸âƒ£ Launch Simulation

```bash
# Terminal 1 - Gazebo
make gazebo

# Terminal 2 - RViz (optional)
make rviz
```

### 4ï¸âƒ£ Start Training

```bash
# Terminal 3 - RL Training
make train
```

### 5ï¸âƒ£ Evaluate Trained Model

```bash
make evaluate MODEL_PATH=training_results_<timestamp>/models/best_model.pth
```

---

## ğŸ¯ Key Features

### ğŸ¦¾ Robot Simulation

* 6-DOF bipedal robot (hip, knee, ankle per leg)
* Gazebo physics with collision dynamics
* ROS2 integration for messaging and control
* RViz for visualization

### ğŸ§  Reinforcement Learning

* PPO (Proximal Policy Optimization)
* Custom Gym-style environment for locomotion
* Reward engineering for:

  * Forward velocity
  * Stability (roll/pitch)
  * Height maintenance
  * Energy efficiency
* Real-time training plots and logging

### âš¡ Advanced Features

* Generalized Advantage Estimation (GAE)
* Gradient clipping for stability
* Automatic model checkpointing
* Multi-episode evaluation and comparison
* Visualization tools (training curves, trajectories, metrics)

---

## ğŸ§  RL Algorithm (PPO)

* **Policy Network**: Actor-Critic with shared feature layers
* **Action Space**: Continuous torques for 6 joints
* **Observation Space**: Joint states + robot pose (18D)
* **Hyperparameters**:

  * LR: `3e-4`
  * Î³: `0.99`
  * Clip Ratio: `0.2`
  * GAE Î»: `0.95`
  * Entropy Coef: `0.01`

**Reward Function:**

```python
reward = velocity_reward + stability_reward + height_reward + survival_bonus
```

---

## ğŸ“Š Training Configuration

* Max Episode Steps: **1000**
* Control Frequency: **50 Hz**
* Target Velocity: **0.5 m/s**
* Termination: fall, tilt, or max steps

**Training Defaults:**

* Episodes: `1000`
* PPO Epochs: `10`
* Save every: `50` episodes
* Plot update: every `25` episodes

---

## ğŸ“ˆ Monitoring Training

### Metrics

* Episode reward
* Episode length
* Policy & value losses
* Robot state

### Example Output

```
Episode 1/1000
  Step 100: Pos=(0.15, 0.98), Vel=0.23, Reward=2.45
  ...
Episode 1 completed:
  Reward: 245.67
  Length: 342
  Final Position: [1.23, 0.0, 0.87]
  âœ… New best model saved!
```

---

## ğŸ§ª Evaluation

Run evaluation:

```bash
python src/rl_bipedal_walking/evaluation/evaluate_model.py \
  --model training_results_20231201_120000/models/best_model.pth \
  --episodes 10 \
  --save-plots evaluation_results
```

Compare multiple models:

```bash
python src/rl_bipedal_walking/evaluation/evaluate_model.py \
  --compare model1.pth model2.pth model3.pth \
  --episodes 5 \
  --save-plots comparison_results
```

**Evaluation Metrics**

* Average Reward
* Success Rate
* Max Distance
* Stability (roll/pitch variance)
* Trajectory visualization

---

## ğŸ› ï¸ Troubleshooting

* **Gazebo wonâ€™t start** â†’ check `gazebo --version`, source ROS2
* **Robot falls immediately** â†’ validate URDF, physics params
* **Training stalls** â†’ lower LR, extend episodes, tune rewards
* **ROS2 issues** â†’ `ros2 topic list`, `ros2 param get ...`
* **CUDA issues** â†’ check `torch.cuda.is_available()`, fallback to CPU

---

## âš¡ Optimization Tips

* Enable GPU training (PyTorch CUDA)
* Run headless (`--no-render`)
* Use vectorized environments
* Curriculum learning & domain randomization
* Gradient clipping + LR scheduling

---

## ğŸš€ Advanced Usage

* Custom robot models: add new URDF & update environment
* Modify rewards: edit `bipedal_env.py`
* Hyperparameter tuning: adjust in `train_walker.py`

---

## ğŸ“œ License

MIT License â€“ free to use, modify, and distribute.

